{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Supervised learning <center>\n",
    "\n",
    "### Linear Regression:\n",
    "In statistics, linear regression is a linear approach for modeling the relationship between a **scalar dependent variable** y and one or more explanatory variables (or **independent variables**) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in linear regression our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](../images/linear_regression/linear_regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the target variable that we’re trying to predict is continuous, such as in housing price prediction, we call the learning problem a regression problem. When y can take on only a small number of discrete values  we call it a classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$h_{\\theta}(x) = {\\theta}_0 + \\sum_{j=1}^n {\\theta}_j x_j = {\\theta}^Tx$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i = 1}^{m} (\\hat{y_i} - y_i)^2 = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_{\\theta}(x) - y_i)^2$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break it apart, it is $\\frac{1}{2}\\hat{x}$ where $\\hat{x}$ is the mean of the squares of $h_\\theta(x_i)−y_i$ , or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the  **Squared error function** or **Mean squared error**. The mean is halved (1/2) as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the 1/2 term. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "So we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where gradient descent comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat until convergence: {\n",
    "    $$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}).x_0^{(i)}$$\n",
    "    $$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}).x_1^{(i)}$$\n",
    "    $$\\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}).x_2^{(i)}$$\n",
    "    $$...$$\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/linear_regression/gradient_descent.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\theta_j} J(\\theta) =  \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}(h_\\theta(x) - y)^2$$\n",
    "$$= 2.\\frac{1}{2}(h_\\theta(x) - y).\\frac{\\partial}{\\partial \\theta_j}(h_\\theta(x) - y)$$\n",
    "$$= (h_\\theta(x) - y). \\frac{\\partial}{\\partial \\theta_j}(\\sum_{i = 0}^{n}{\\theta}_i x_i - y)$$ \n",
    "$$= (h_\\theta(x) - y).x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally\n",
    "![](images/linear_regression/all.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: $$h_{\\theta}(x) = {\\theta}_0 + \\sum_{j=1}^n {\\theta}_j x_j = {\\theta}^Tx$$\n",
    "Gradient = derivative of (h - y): $$J(\\theta) = \\frac{1}{2}\\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "Least Means Squares Update Rule: $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\vec{\\theta})$$\n",
    "$$= \\theta_j - \\alpha\\frac{1}{m}\\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/linear_regression/visu1.gif?raw=true)\n",
    "![](../images/linear_regression/visu2.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune learning rate\n",
    "\n",
    "\n",
    "If α is too small: slow convergence.\n",
    "\n",
    "If α is too large: ￼may not decrease on every iteration and thus may not converge.\n",
    "\n",
    "![](../images/linear_regression/learning_rate.png?raw=true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
